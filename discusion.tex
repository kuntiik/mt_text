\chapter{Discusion and further suggestions}
\label{chapter:discusion}

\section{Coments on model comparison and their improvements}
We observed that updating the dataset by either increasing the number of data or correcting annotations mistakes showed significant improvement in monitored metrics.

The results differ only in units of percents if we use models with smaller backbones as can be seen in Table \ref{tab:backbone_comparison}, furthermore parameter-heavy models such as EfficientDet-D4 performed worse than others, see Table \ref{tab:model_results:stage_four}. This was a surprising fact, since EfficentDet-D4 was out-performing all other tested models on the MS COCO benchmark \cite{paperwithcode,Tan2019}.


The weight decay did not affect the behavior of YOLOv5 and Faster R-CNN. This was surprising since we would expect an increase in performance on the validation and test dataset.

The use of group normalization significantly decreased the performance gap between EfficiendDet and other models (Section \ref{sec:group_nromalization:res }), but the results were still inferior to most of the models.


\section{Ensembling}
In tables \ref{tab:precision:grid_search} and \ref{tab:recall:grid_search} we observe, that the best performing method was WBF. The other methods showed smaller but still significant improvement. This corresponds to findings by Solovyev et al. \cite{Solovyev2019}. We were surprised that extending the functionality of WBF by reweighting predictions according to their area did not improve the performance of the ensembled model.

From tables \ref{tab:model_results:stage_four} and \ref{tab:model_ensembling:handpicked} we see, that $AP@.5$ obtained by ensembled models improved over the best performing model by $2.3\%$ when we manually selected hyper-parameters and used WBF method. On the contrary, models ensembled by parameters found by grid search improved by $6.7\%$ in terms of $AP@.5$. This emphasizes the importance of the hyper-parameter search for ensembling methods.

Tables \ref{tab:precision:ensemble_compare} and \ref{tab:recall:ensemble_compare} show the importance of using different architectures and backbones in ensembling. We found that the usage of varying backbones increases the gain in average precision by circa $1.5\%$. Furthermore, using different architectures increased the performance by an additional $3.2\%$. Please note that models included in ensembling with different architectures had a better average $AP@.5$ by $2\%$ higher than models included in the former two ensembling approaches. If we adjust the results, we expect a $1.2\%$ gain solely from using models with varying architecture.

In figure \ref{fig:ensembling_parameters_importance} we see that the second worst-performing model included in the enabling had the most significant importance for the ensemble. This is a surprising fact, but it underlines the previous findings that varying models contribute more to performance gains of ensembling.

\section{Dental restorations segmentation}
\subsection{Non-deep learning approach}
The grid search did not find a hyper-parameter, ensuring that adaptive thresholding methods would detect only dental restoration. From the figure \ref{fig:segmentation_sample_nondl} we see that adaptive thresholding methods include a great amount of false-positive predictions. A significant amount of those is removed by morphological operations, but the IOU 0.314 is still relatively low compared to U-Net models. This corresponds to the results of Abdalla-Aslan et al. \cite{AbdallaAslan2020}, who achieved a precision of 0.33 when segmenting dental restorations from panoramic images.
\subsection{U-Net}
The baseline U-Net model already showed significant performance gain over the non-deep learning segmentation pipeline. It was by more than $10\%$ by using postprocessing by morphological operations.

Improvements in the training process increased the performance of U-Net significantly; on the contrary, post-processing the improved U-Net model improved all tracked metrics negligibly.

In the figure \ref{fig:segmentation_unet_sample} we see a comparison of the predicted pixel mask with the ground truth. In the border areas of dental restorations, the model seems to estimate its position better than the ground truth labels. This happens due to the unease of labeling data for segmentation tasks, where the annotator needs to include many points in the bounding rectangle to correspond to the real position of the restoration. We believe that this hurts the reported performance of the model.



\section{Comparision of results with related publications}

Results obtained by this work have beaten those achieved by Srivastava at al. \cite{Srivastava2017} and Kumar and Srivastava \cite{Kumar}. Please note that even though Kumar continued on the work pubulished by Srivastava et al., even extending the dataset used by Srivastava twofold, all metrics reported by them dropped significantly. We cannot explain what caused this decrease in performance. Kumar does not adress this problem in the published paper.

Bayrakdar2021 at al. \cite{Bayrakdar2021} and Bayraktar2021 et al \cite{Bayraktar2021} reported better results, than we achieved througout our work. We find that surprising since both works had significantly smaller ammount of data (621 and 1000 images). We experimented with the YOLOv3 architecture used by \cite{Bayraktar2021}, but in our experiments, it achieved worse results than other architectures. We, therefore, found their results to be irreproducible.

Cantu et al. achieved a similar F1-Score as we did in our work. The comparability of those results is limited since they solved semantic segmentation tasks contrary to object detection.
