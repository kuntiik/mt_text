\chapter{Discusion and further suggestions}
\section{Baseline model comparison}
\subsection{Stage one dataset}
In the first stage, non of the trained model's none of the models performed well. Especially Faster-RCNN with ResNet50 backbone achieved low average precision values. We attribute it to inhomogeneity in the dataset and the low amount of data.

\subsection{Stage two dataset}
Even though the dataset grew by more than 50\% since stage one, there was only mediocre improvement in the average precision of the models. On the other hand, from the table \ref{tab:model_results:stage_two} we could see that the EfficientDet model was able to fit the training part of the dataset well.

\subsection{Stage three dataset}
Revisiting and correcting annotation errors of the stage two dataset improved $AP@.5$ of EfficientDet-D4 by $76\%$, although the amount of data remained the same. This gives us an intuition that mistakes in annotations of the data primarily caused the low average precision achieved on stage one and two datasets.

\subsection{Stage four dataset}
Despite the dataset increasing by another $35\%$ of images available in stage three, the increase in performance was significantly smaller than the one we got by transitioning from stage two dataset to stage three. We hypothesize that by removing the errors in the dataset, the impact of obtaining more data became smaller. It is interesting that all models were almost on-par in terms of average precision, the only difference being the EfficientDet-D4 model, which was by circa $10\%$ worse than the rest. This somehow surprised us since EfficientDet-D4 was the best performing model on the MS COCO benchmark from all models that we used \cite{paperwithcode}. We attribute this to this model's high GPU memory requirements, which allowed us to use batch-size of one only.

\subsection{Stage five dataset}

Stage five dataset brought a steady improvement according to our expectations. The well-performing models in stage four performed well in stage five, the only difference being the YOLOv5-l6 model, which was circa $15\%$ worse than the rest. We think this was due to a bad model fit and not its incapability to perform well.

\section{Model improvements}
\subsection{General improvements}
From the table \ref{tab:improved:precision} we see, that the best-performing did not improved when compared to results in table \ref{tab:model_results:stage_five}. On the other hand, all models, on average, converged to better results.

\subsection{Group normalization}
Even though group normalization significantly improved the results of the EfficientDet-D5 model. It still performed on par with other architectures at best. We, therefore, decided not to use EfficientDet-D5 and D4 models since they were slightly worse than the rest and took a long time to train.

\section{Model inspection}
\subsection{Size of backbone}
Even though the difference in the performance of different backbones differed only slightly, we could see a trend where a bigger backbone performs better. This holds in terms of maximal achieved performance as well as mean performance across multiple training instances.

\subsection{Weight decay}
The value of weight decay seemed to not affect the model's performance. The $AP$ of models with different value weight decay did not even differ during the training. This was surprising since we would expect a decrease in the generalization gap.

\section{Ensembling}



