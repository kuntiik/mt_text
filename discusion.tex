\chapter{Discusion and further suggestions}
\section{Baseline model comparison}
\subsection{Stage one dataset}
In the first stage, non of the trained model's none of the models performed well. Especially Faster-RCNN with ResNet50 backbone achieved low average precision values. We attribute it to inhomogeneity in the dataset and the low amount of data.

\subsection{Stage two dataset}
Even though the dataset grew by more than 50\% since stage one, there was only mediocre improvement in the average precision of the models. On the other hand, from the table \ref{tab:model_results:stage_two} we could see that the EfficientDet model was able to fit the training part of the dataset well.

\subsection{Stage three dataset}
Revisiting and correcting annotation errors of the stage two dataset improved $AP@.5$ of EfficientDet-D4 by $76\%$, although the amount of data remained the same. This gives us an intuition that mistakes in annotations of the data primarily caused the low average precision achieved on stage one and two datasets.

\subsection{Stage four dataset}
Despite the dataset increasing by another $35\%$ of images available in stage three, the increase in performance was significantly smaller than the one we got by transitioning from stage two dataset to stage three. We hypothesize that by removing the errors in the dataset, the impact of obtaining more data became smaller. It is interesting that all models were almost on-par in terms of average precision, the only difference being the EfficientDet-D4 model, which was by circa $10\%$ worse than the rest. This somehow surprised us since EfficientDet-D4 was the best performing model on the MS COCO benchmark from all models that we used \cite{paperwithcode}. We attribute this to this model's high GPU memory requirements, which allowed us to use batch-size of one only.

\subsection{Stage five dataset}

Stage five dataset brought a steady improvement according to our expectations. The well-performing models in stage four performed well in stage five, the only difference being the YOLOv5-l6 model, which was circa $15\%$ worse than the rest. We think this was due to a bad model fit and not its incapability to perform well.

\section{Model improvements}
\subsection{General improvements}
From the table \ref{tab:improved:precision} we see, that the best-performing did not improved when compared to results in table \ref{tab:model_results:stage_five}. On the other hand, all models, on average, converged to better results.

\subsection{Group normalization}
Even though group normalization significantly improved the results of the EfficientDet-D5 model. It still performed on par with other architectures at best. We, therefore, decided not to use EfficientDet-D5 and D4 models since they were slightly worse than the rest and took a long time to train.

\section{Model inspection}
\subsection{Size of backbone}
Even though the difference in the performance of different backbones differed only slightly, we could see a trend where a bigger backbone performs better. This holds in terms of maximal achieved performance as well as mean performance across multiple training instances.

\subsection{Weight decay}
The value of weight decay seemed to not affect the model's performance. The $AP$ of models with different value weight decay did not even differ during the training. This was surprising since we would expect a decrease in the generalization gap.

\section{Ensembling}
In tables \ref{tab:precision:grid_search} and \ref{tab:recall:grid_search} we observe, that the best performing method was WBF. The other methods showed smaller but still significant improvement. This corresponds to findings by Solovyev et al. \cite{Solovyev2019}. We were surprised that extending the functionality of WBF by reweighting predictions according to their area did not improve the performance of the ensembled model.

From tables \ref{tab:model_results:stage_four} and \ref{tab:model_ensembling:handpicked} we see, that $AP@.5$ obtained by ensembled models improved over the best performing model by $2.3\%$ when we manually selected hyper-parameters and used WBF method. On the contrary, models ensembled by parameters found by grid search improved by $6.7\%$ in terms of $AP@.5$. This emphasizes the importance of the hyper-parameter search for ensembling methods.

Tables \ref{tab:precision:ensemble_compare} and \ref{tab:recall:ensemble_compare} show the importance of using different architectures and backbones in ensemble.
We found that the usage of different backbones increases the gain in average precision by circa $1.5\%$. Furthermore, using different architectures increased the performance by an additional $3.2\%$. Please note that models included in ensembling with different architectures had a better average $AP@.5$ by $2\%$ higher than models included in the former two ensembling approaches. If we adjust the results for that, we expect circe $1.2\%$ gain solely from the usage of different architecture models. Figure \ref{fig:ensembling_parameters_importance} we see that the second worst-performing model included in the enabling had the most significant importance for the ensemble.


\section{Dental restorations segmentation}
\subsection{Non-deeplearning approach}





\section{Comparision of results with related publications}

Results obtained by this work have beaten those achieved by Srivastava at al. \cite{Srivastava2017} and Kumar and Srivastava \cite{Kumar}. Please note that even though Kumar continued on the work pubulished by Srivastava et al. even extending the dataset used by Srivastava twofolds, all metrics reported by them dropped significantly. We cannot explain what caused this decrease in performance. Kumar does not adress this problem in the published paper.

Bayrakdar2021 at al. \cite{Bayrakdar2021} and Bayraktar2021 et al \cite{Bayraktar2021} reported better results, than we achieved througout our work. We find that surprising since both works had significantly smaller ammount of data (621 and 1000 images). We experimented with the YOLOv3 architecture used by \cite{Bayraktar2021}, but in our experiments, it achieved worse results than other architectures. We, therefore, found their results to be irreproducible.

Cantu et al. achieved a similar F1-Score as we did in our work. The comparability of those results is limited since they solved semantic segmentation tasks contrary to object detection.
