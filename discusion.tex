\chapter{Discusion and further suggestions}
\section{Discusion}
From the results in the chapter \ref{chapter:results} and figures in the appendix \ref{appendix:model_predictions}, we can see that the model is able to localize large portions of dental caries in the image, yet there is still a room for improvement.

\begin{itemize}
    \item In the figure \ref{fig:yolov5_map_iou_thresholds}, we can see a sharp drop in the MAP values when the IOU threshold greater than 0.4 is chosen. The drop is in both training and validation MAP. This seems to be related to the data rather than to the model. This matches the expectations of Dr.Tichy, who said that in many cases, it is not crystal clear where exactly the tooth decay is located, and there is thus slight ambiguity in the image labeling.
    \item From predictions in figures \ref{fig:pred_img1}, \ref{fig:pred_img2}, \ref{fig:pred_img3}, \ref{fig:pred_img4}, we can see, that the model is incapable of precise predictions in the vicinity of dental restorations. This is common pattern accros the whole dataset and is probably caused by high variety of restorations shapes and by the low ammount similar images in the dataset.
    \item The best performing YOLOv5 backbone was not the biggest one. Even though, the difference was negligible; it is still an unexpected result. The 5x6 backbone achieved by 10 \% better results than 5m6 when benchmarked on the MS COCO dataset. The unability to utilize the bigger the network can be caused by wrongly setup training pipeline or the low amount of data, when smaller network is not able to over-fit to the training dataset.
    \item There was a drop of the model performance when selecting the image size of 1024 over 896. Even though the difference is subtle, I would expect an improvement, rather than drop of performance.
    \item Even thought in the MS COCO benchmark EfficientDet-D4 is better performing model than YOLOv5-5l6, it has shown to be worse performing on our dataset. This could be caused by extremely low batch size of 1.
\end{itemize}
\section{Suggestions for further work}
Right now, there seems to be no clear way how to improve the performance significantly. We could try to do the hyper-parameter search, but personally I would resort to this option latter.

We could try different architectures since the best performing YOLOv5 is not a state-of-the-art \cite{paperwithcode}. YOLOR and the newest versions of YOLOv4 are outperforming YOLOv5 by 12\% and 10\%, respectively.

We could modify the training pipeline, for instance, changing the optimizer and learning rate scheduler.

We could try to do image preprocessing or try automatic augmentation search as proposed by Cubut et al. \cite{Cubuk2018}.

Methods that almost always improve the performance are model ensembling and test-time augmentations, we could try how big performance bump, they would provide in our problem.