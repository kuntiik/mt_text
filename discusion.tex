\chapter{Discusion and further suggestions}
\section{Discusion}
From the results in chapter \ref{chapter:results} and figures in appendix \ref{appendix:model_predictions}, we can see that the model is able to localize large portions of dental caries in the image, yet there is still a room for improvement.

\begin{itemize}
    \item In figure \ref{fig:yolov5_map_iou_thresholds}, we can see a sharp drop in the MAP values when IOU threshold greater than 0.4 is chosen. It is observable in both training and validation MAP. This seems to be related to the data rather than to the model. Also, it matches the expectations of Dr.Tichy, who said that in many cases it is not crystal clear where exactly the tooth decay is located, and there is thus slight ambiguity in the image labeling.
    \item From predictions in figures \ref{fig:pred_img1}, \ref{fig:pred_img2}, \ref{fig:pred_img3}, \ref{fig:pred_img4}, we can see that the model is incapable of precise predictions in the vicinity of dental restorations. This is a common pattern accros the whole dataset and is probably caused by a high variety of restorations shapes as well as by the low amount of similar images in the dataset.
    \item The best performing YOLOv5 backbone was not the biggest one, but the 5m6 backbone, which is considered to be a medium sized option. Even though the difference was negligible, it is still an unexpected result, that the 5m6 backbone was albe to outperform 5x6 backbone, even though it achieved by 10 \% better results than 5m6 when benchmarked on the MS COCO dataset \cite{glennjocher2020}. The inability to utilize the bigger backbone can be caused by a wrong setup of the training pipeline or the low amount of data.
    \item There was a drop of the model performance when selecting the image size of 1024 over 896. Even though the difference is subtle, we would expect the opposite.
    \item Even though in MS COCO benchmark EfficientDet-D4, there is a better performing model than in YOLOv5-5l6, it has shown to be worse performing on our dataset. This could be caused by extremely low batch size of 1.
\end{itemize}
\section{Suggestions for further work}
Right now, there seems to be no straightforward way to improve the performance significantly. We could try to do the hyper-parameter search, but personally I would resort to this option later.

We could try different architectures since the best performing YOLOv5 is not state-of-the-art \cite{paperwithcode}. YOLOR and the newest versions of YOLOv4 are outperforming YOLOv5 by 12\% and 10\%.

We could modify the training pipeline by, for instance, changing the optimizer and the learning rate scheduler.

We could try to do image preprocessing or try automatic augmentation search as proposed by Cubut et al. \cite{Cubuk2018}.

Methods that almost always improve the performance are model ensembling and test-time augmentations. We could try how big performance bump they would provide in our problem.